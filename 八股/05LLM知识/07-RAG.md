# RAG

1. 检索  
    - 索引优化：
        - 固定长度切分
        - 根据语义进行切分
        - 使用默认分隔符（如。）进行切分
        - late chunking：之前是先chunk再embedding，语义向量局部在chunk中；而late chunking是先embedding再chunk，这样embedding的信息会更加充分(JINA)
        - 利用LLM将分块和完整文档一起传给LLM，LLM给chunk显示添加上下文信息(Anthropic)
    - 查询优化：
        - HyDE：第一步让LLM针对query生成多个假设文档。第二步对生成的多个假设文档进行向量编码之后，用它们的平均向量去检索最相似的文档。在计算平均向量时，也可以考虑将原始query的向量作为其输入向量的一部分。
        - BM25：BM25的核心思想是基于词频(TF)和逆文档频率(IDF)来，同时还引入了文档的长度信息来计算文档D和查询Q之间的相关性。
            - **词频**：这是查询中的词 q_i在文档 D 中出现的频率。
            - **逆文档频率**：一个词在很多文档中出现，其IDF值就会低，反之则高。这意味着罕见的词通常有更高的IDF值，从而在相关性评分中拥有更大的权重。
    - Embedding：
        - M3E：
        - BGE：
    - Reranker（精筛）：
        - 对多路召回的数据（粗选），进行归一化重排序。效果比Embedding的相似度强

2. 生成
    - query改写

# GRAPHRAG

# Search-O1【Agentic Search-Enhanced Large Reasoning Models】
> https://arxiv.org/abs/2501.05366


# Search-R1【通过强化学习让模型具有自主搜索能力】
> https://arxiv.org/pdf/2503.09516
> https://zhuanlan.zhihu.com/p/30784344002

本文尝试将强化学习框架和检索增强场景想结合。**核心思想：将搜索引擎建模为强化学习环境的一部分，使LLM能通过试错自主学习，打破现有的 RAG 和工具使用方法在灵活性和可扩展性方面存在局限性【如现在的deep research项目】。**

搜索即环境交互：将搜索 API 调用转化为马尔可夫决策过程的状态转移  
结构化文本生成：通过特殊标记实现程序化控制  
轻量化奖励设计：仅用最终答案正确性作为奖励信号，避免复杂过程监督  

## 例子
```
<think>xxxx</think>
<search>xxx</search>【当检测到模型生成这个特殊token时进行检索】
<information>xxx</information>

...

<answer>xxxx</answer>
```

## 奖励模型
规则奖励，精准字符匹配

## QA
Q1：SEARCH-R1 如何解决多轮交互的问题？
SEARCH-R1 的核心创新在于它允许 LLM 在推理过程中自主决定何时以及如何进行搜索。通过引入 <search> 和 </search> 等特殊 token，LLM 可以在生成文本的过程中，显式地发出搜索请求。系统会根据这些 token 提取查询语句，调用搜索引擎，并将检索结果以 <information> 和 </information> 的形式插入回 LLM 的上下文。

这样，LLM 就可以根据实时检索到的信息，动态调整其推理路径和搜索策略，实现真正意义上的多轮交互。

Q2：检索 Token 掩码（retrieved Token masking）的作用是什么？为什么它可以提高训练的稳定性？
在强化学习中，模型的目标是通过调整策略来最大化奖励。如果没有检索 token 掩码，模型可能会尝试直接修改检索到的文本，以获得更高的奖励。这会导致以下问题：

信息污染：模型可能会改变检索到的信息，使其与事实不符，从而损害模型的知识来源。
训练不稳定：模型可能会过度关注检索到的文本，而忽略了自身推理能力的提升。
检索 token 掩码通过阻止模型修改检索到的 token，强制模型专注于如何更好地利用这些信息。这可以避免信息污染，并促使模型学习更有效的推理和搜索策略，从而提高训练的稳定性。

Q3：为什么文章选择使用基于结果的奖励函数，而不是基于过程的奖励函数？
基于过程的奖励函数需要对模型的中间推理步骤进行评估，这通常需要人工标注或复杂的规则。这不仅增加了成本和复杂性，而且也容易引入人为偏见。

基于结果的奖励函数只关注最终答案的正确性，避免了对中间步骤的评估，更加简单和通用。文章的实验结果表明，即使只使用简单的基于结果的奖励函数，SEARCH-R1 也能有效地学习到复杂的推理和搜索策略。

Q4：为什么要用强化学习 + 搜索而不是监督学习 + 搜索？
技术细节：

多轮搜索的决策树复杂度呈指数增长（3 轮搜索可能有 $10^6$ 种路径），监督学习难以覆盖所有情况
强化学习的试错机制能自主发现最优搜索策略，如论文表 5 案例中模型自动进行验证性检索
就像教小孩骑自行车，监督学习相当于在训练轮上反复练习固定动作，而强化学习允许他自主探索平衡点，通过摔倒（负奖励）和前进（正奖励）来学习更灵活的操控。




# R1-Searcher
> https://arxiv.org/pdf/2503.05592


