# 前身COT

COT（chain-of-thought）本身属于prompt工程，通过写出更好的prompt来激发模型回答出正确答案的能力

## COT
> https://arxiv.org/pdf/2201.11903
![COT_example](cot_example.png)
把思考的中间过程详细地写在prompt中，引导模型也进行同样的思考输出，提升推理能力

## COT self-consistency
> https://arxiv.org/pdf/2203.11171
![cot_self-consistency](cot_self-consistency.png)
多模型的结果进行多次采样，采用策略（如投票法），选择最好的结果

## zero-shot COT（Let's think step by step）
> https://arxiv.org/pdf/2205.11916
![Let's think step by step](zero-shot_cot.png)
通过一个简单的命令 let's think step by step，不需要shot就可以让模型进行思考推导，并得到正确答案，减少手工cot的成本

## Auto COT
> https://zhuanlan.zhihu.com/p/17878862043
![Auto COT](auto_cot.png)
从一堆问题集中抽取适合的问题（论文中选择的标准是：简单、问题短、推理步骤少，防止模型推理出错给用户提问一个错误的shot）用let's think step by step引导模型进行解答得到qa对（zero-shot cot），然后将qa对作为shot来完成用户提问的解答

## TOT 
> https://zhuanlan.zhihu.com/p/634180290
> https://arxiv.org/pdf/2305.10601
![tot](tot.png)
1. 将一个问题拆解为多个阶段来进行回答
2. 模型每个阶段生成多个样本
3. 评价模型上一步生成的哪个样本最好

## 引申：如何在写作中引入COT，提升正文质量
idea：用r1造数据集对<think>部分进行人工校对，然后训练自己的模型，使其拥有高质量的think（推理）能力

## “Wait”注入
> https://github.com/simplescaling/s1?tab=readme-ov-file#vllm-with-budget-forcing
> https://zhuanlan.zhihu.com/p/21602993558  

通过注入wait，延长模型的思考，变相增加推理时间
```python
o = model.generate(prompt)

ignore_str = "Wait" # 注入词wait
max_tokens_thinking_tmp = MAX_TOKENS_THINKING
if max_tokens_thinking_tmp > 0:
    for i in range(NUM_IGNORE): # 注入几次
        max_tokens_thinking_tmp -= len(o[0].outputs[0].token_ids)
        prompt += o[0].outputs[0].text + ignore_str # prompt拼接模型返回+wait
        o = model.generate(prompt)  # 模型看见wait会进行反思，生成更好的结果
```


# o1最初的推理模型

> https://zhuanlan.zhihu.com/p/720106482  
> https://zhuanlan.zhihu.com/p/13872128423

o1引入 推理时计算 *test-time compute* 概念

推理模型的主要创新点：模型可以自己提出假设、验证假设、进行反思（如果假设结果错误）【模型具有质疑自己前文推断错误的能力】

openai声明在训练时使用了RL技术来实现o1

## 为何想到RL技术
SFT走到了尽头，训练数据几乎用光，开始使用RL技术提升模型效果
> Scaling Law是一个描述 LLM 的测试损失随某个量（如训练计算量、模型参数、数据集大小）的增长而降低的公式  
> scaling law指导了loss的下降，但是loss下降不一定性能提升（如：思考能力）,pretrain逐渐失效,模型能力达到瓶颈
> https://zhuanlan.zhihu.com/p/20966132534

SFT还存在的问题：
1. 需要大量的高质量数据，难以应用到垂类领域
2. 无法使用负样本

## 为什么RL技术可以赋予模型推理能力

### Self-paly（o1可能使用的技术）
| 技术 | 优点|缺点|模型|
| --- | ---|---|---|
| RLHF	| 1. 可以对齐人类偏好及价值观<br />2. 能利用错误数据<br />3. 数据利用效率高 |1. 偏好建模困难，容易hacking<br/>2. 训练成本高 |ChatGPT|
| Self-play	| 1. 绝对强度更高，甚至超越最强人类、专家<br/>2. 可以实现双人零和博弈的最优 |1. 有时候无法理解人类，行为不像人<br/>2. 训练及推理成本极高 |o1|  

RLHF训练得到的ChatGPT好像是一个死记硬背的书呆子，推理能力迟迟没有见到突飞猛进的变化。  

强大的verifier（RM模型）是self-paly的基础，在Self-paly中generator（LLM） 和 verifier（RM模型）交互对抗、互相增强（类似GAN）提升性能。  
generator可能通过树状搜索，进行多分支推理过程【test time increasing】，verifier负责对不好的路径进行剪枝，至最终推理答案。

### r1的强化学习方法

> https://zhuanlan.zhihu.com/p/20538667476  
> https://zhuanlan.zhihu.com/p/19868935152  
> https://zhuanlan.zhihu.com/p/22103896675  
> https://arxiv.org/pdf/2501.12948

![r1_train_process](r1_train_process.jpg)

原文个人总结：  
v3 ->(RL) ZERO




# 总结：模型推理能力的产生
通过强化学习技术xxx