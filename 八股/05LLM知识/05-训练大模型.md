# 训练（微调）大模型

## 拒绝采样


## LORA
LoRA的本质就是用更少的训练参数【低秩，通过一个较低维度的表示来近似表示一个高维矩阵】来近似LLM全参数微调所得的增量参数，从而达到使用更少显存占用的高效微调。

LoRA的做法是在LLM的某些矩阵（ $\boldsymbol{W} \in \mathbb{R}^{d \times k}$ ）旁插入一个和它并行的新的权值矩阵（$\Delta \boldsymbol{W} \in \mathbb{R}^{d \times k}$），但是因为模型的低秩性的存在，我们可以将 $\Delta \boldsymbol{W}$拆分成降维矩阵（$\boldsymbol{A} \in \mathbb{R}^{r \times k}$） 和升维矩阵（$\boldsymbol{B} \in \mathbb{R}^{d \times r}$），其中r很小，从而实现了以极小的参数数量训练LLM。

![alt text](lora.png)

