# 工作流式（workflow）的多章节并行文章生成【deep research】

技术方案的变化历程
首先，文章生成的流程为 用户输入标题 -> 大纲生成 -> 研究思路生成 -> 正文生成（搜索 + 写作）

1. 大纲生成
    1. **真实研报抽取+改写**：
        之前的问题：
        改进：显然应该学习真实研报，但是真实研报的标题形容词丰富、都是作者基于正文润色的（如：智驾平权落地，品牌势能向上 -> 智能驾驶技术研发情况；大力改革，苦修内功 -> 管理层与股权结构分析），而我们项目的大纲标题初始生成的时候是一个无观点的标题（xx公司基本面分析、xx行业分析），所以用LLM对真实研报的大纲进行抽取改写得到数据集。【真实研报的抽取 minerU工具（PDF解析工具）进行文档解析，然后利用模型进行标题识别，得到一二级标题的大纲】
    2. **维度全面、补充特定公司资料**：
        之前的问题：真实研报集中往往分析某些利好的方面（因为作者在写作之前已经知道要写什么），所以真实研报的维度是不全面的。
        改进：因为我们不知道要提前研究什么，所以期望生成维度全面的大纲结构。这里我们取上市公司最近的中年报中经营分析章节【这里使用了GRAPH_RAG的技术#TODO】，加入PROMPT中全维度分析公司的各个方面，所以我们用GPT对原数据集改造+人工校对的方式得到了新数据集。
    3. **在正文生成结束润色形成带观点**：
        之前的问题：大纲标题过于直白，没有观点。（正如第一点提到的）
        改进：基于正文对标题进行润色，发现R1生成的结果语言风格与专业性表现都很好、且这种简单任务也没有幻觉问题，用R1做模型蒸馏构建数据集。
    4. **专题报告（大纲不是维度全面的、而是专注的）生成**：
        之前的问题：用户可能不想对公司做全维度的分析，只想分析公司的营收、某个业务的发展情况等等专题研究。（线上之后收集用户输入的标题发现的问题）
        改进：开始的时候用GPT构建专题大纲数据集但是效果不好，改成r1模型之后效果大幅提升，专题报告的专业性强。
    5. **支持用户指令输入【仿OPENAI-deep research】**：
        之前的问题：1. 用户目前只能上传标题 或者 自己的大纲，不能写一段话来进行大纲生成，期望新的交互更智能、更像AI产品。 2. 用户输入的信息太少了、目前只有标题。
        改进：1. 仿OPENAI的交互模型，用户输入->模型澄清->用户反馈->大纲生成 2. 在大纲这一步将研究思路一并生成，各个标题下的研究思路聚焦于用户的反馈 3. 研究思路从之前的分点列举改为一段逻辑性更强的文字，指导正文生成（deep research模式）。3. 在生成大纲之前进行信息搜索，这也是之前没有的


2. 研究思路生成（RAG搜索+embedding -> Agent式串行 -> deep search深度搜索思考（受openai启发，在核对时传入高质量且有效的研究思路））
    1. **RAG搜索+embedding**：
        之前的问题：
        改进：用title检索网页、切分、相关性判断、LLM生成关键词作为研究思路。
    2. **Agent式串行 ReACT**：
        之前的问题：研究思路的粒度太粗，仅分成了几个宽泛的点
        改进：设计了二级的研究思路结构从粗到细，并结合了Agent进行搜素工具的调用
    3. **deep search深度搜索思考**：
        之前的问题：同上第5条
        改进：xxx

3. 搜索（公司内部的搜索接口 和 博查、秘塔等有差距），总结与过滤（从网页分段embedding -> LLM总结）
    - 金融指标数据的搜索
        - 什么是金融指标（以时间为序列的数值）
        新能源汽车:产量:当月值、GDP:全国
    
        - 为什么用金融指标
        核心是画图，然后可以让正文素材来源引用规范、真实，体现公司数据丰富
    
        - 如何检索出适合的金融指标
        （embedding（没有联想能力、embedding效果相关性也不佳） -> LLM（按行业构建指标库【人工+LLM】、利用真实研报的图文信息【失败】） -> 正文写完反向找指标增强【利用R1推理能力，生成聚合指标图】）

    - 搜索接口即是Agent工具
        - 公司公告
        - 研究报告
        - bing
        - 金融指标
        - 公司数据接口（分产品营收、公司股权结构）
    
    - 总结与过滤
        - 总结 + 补充质量、主题一致性判断（GPT判断得没有很准，例：三星 兴三星）（从网页分段embedding -> LLM总结）

4. 正文生成
    - GPT蒸馏 -> R1蒸馏（幻觉多+AI校对、人工校对复杂）

5. 特定章节写作
    - 投资建议（重要章节）
    搜索大概率找不到合适的素材、如：PE、分产品营收、同比公司
    - 风险提示（不重要章节）
    章节短、不必搜索


# 用户金融指标查询【GRPO优化】
query -> search    -> subquery(GRPO) -> Embedding（Top50） -> 编辑距离补充 + 精筛（最多 Top5）
      -> no search -> Embedding

## 例子
query：汽车上游金融成本价格 -> 

search：1. search query：汽车产业上游金融有那些 2. 检索的文本xxxx -> 

subquery：铝价格、铜价格、xxx -> 

Embedding：上海期货交易所:铝:每日价（20个指标） -> 

（编辑距离：补充查找同比、环比等信息 -> ）

精筛：LLM（复合指标聚类召回）

## 问题
1. 如何判断search 和 no search
一个prompt完成拆分判断+search query的生成。LLM做得不错，没有大问题，蒸馏数据。

search query有人工的修改，也有自动生成修改（LLM改写5个query，返回搜索结果之后进行对比，选搜索结果最好的query）。

2. subquery的训练
数据集：人工 + 自动合成（金融指标的自然语言 -> LLM生成多个query，取embedding分数最高的）

3. embedding的训练
bge-base + 正负样本训练（人工标注）

4. 评价
人工评价（好 中 差） + 自动评价（相似度分数（好 中 差） + 和上一个版本的对比）

## GRPO优化

奖励函数：格式生成 + 长度惩罚 + 召回率

现象：1. reward，先比较陡峭地上升，然后平滑波动 2. 在平滑波动期间，模型生成的长度（即keyword的长度）变短，任务是长度惩罚在起作用。

# DEEP RESEARCH 金融研报写作
背景：券商的研究员写作研报获取收益，我们期望用AI替代人类完成写作，产生价值。  
方案：workflow工作流式的deep research，通过用户输入 澄清 大纲 写作逻辑 deep research 正文生成

## 澄清+用户反馈

是否训练？否  
为什么没有训练？大模型基座的表现足够强  
输入和输出是什么？输入是用户随意写的写作要求，输出是AI生成的澄清问题

亮点：1. 线上发现用户不爱澄清，后面让模型生成多个选项，用户做选择而不是写，同时保留一个写的窗口（每个问题的回答 + 一个用户额外补充的问题）  
**QA**  
问1：那为什么希望用户做澄清，而不是跳过澄清  
答1：1. 产品层面，给用户感觉是一个更智能的系统 2. 内容通过用户澄清聚焦之后，写作出来的文字会更有深度、主题更聚焦（广度搜索少一些，深度搜索多一些）。

## 大纲

是否训练？是  
训练的数据集是怎么来的？从真实研报中抽取【】+人工校对确保数据集质量高+一直以来积累的用户数据
数据集样本数？1000 SFT + 100 DPO  
训练是怎么做的？第一阶段SFT + 第二阶段DPO（格式DPO，负例为3级大纲、超长/短大纲）  
输入和输出是什么？输入是用户写作要求 + 澄清 + 用户反馈，输出是两级标题结构的大纲  

亮点：1. 回流线上badcase做dpo，正样本来自人工标注  
细节：1. 做LORA训练时，SFT的LORA adapter会被DPO用来继续在其上进行训练

## 研究目的（写作逻辑）

是否训练？是  
训练的数据集是怎么来的？从真实研报中抽取（以一级标题抽取整个二级标题结构的研究目的）+人工校对确保数据集质量高  
数据集样本数？2000 SFT  
训练是怎么做的？第一阶段SFT  
输入和输出是什么？输入是用户写作要求 + 澄清 + 用户反馈 + 大纲，输出对应一级标题下所有二级标题的写作逻辑  

亮点：  对于一个从真实研报中抽取的过程，先进行一级标题抽取，然后多个一级标题的研究目的进行内容校对、去重、逻辑前后连贯性的梳理。
**QA：**   
问1. 为什么不能够全部大纲的写作逻辑一次性生成？  
答1：a. 因为太长了，20（7*3）个二级标题，每个500字，超长输出的训练困难，虽然可以写得长，但是模型的表现不好（这里我们也看过一篇论文LONGWriter GLM智谱团队做的 他们做了一个超长数据集让模型生成得足够长，但是实验结果也不好，模型能输出得长，但是质量并没有提升，反而下降）   
b. 从真实研报中用R1 GPT-4o等模型从超长报告中抽取写作逻辑也比较困难，模型经常输出的不够长，忽略后面的章节；而用人工做这件事情又不现实  
c. 从产品上，希望第一个字可以更快地流出来 

问2：为什么大纲和研究目的不一起生成？  
答2：我们纯从产品角度考虑，希望马上生成大纲流出来给用户看，不让用户等待，因为我们系统本身的显卡是A10，支持的速度不快

一个研究目的的例子：
首先，从生猪存栏量和能繁母猪数量等核心指标入手，梳理近年来供给端的周期性波动特征，明确当前处于周期的哪个阶段。其次，深入剖析供给端的结构性变化，包括养殖主体结构的演变（如散户退出、规模化养殖加速）、养殖效率的提升（如饲料转化率、出栏周期等）以及区域布局的调整（如环保限养区转移、养殖集中化趋势）。最后，探讨外部因素对供给端的潜在影响，例如饲料成本、疫病防控水平、养殖技术进步等，评估其对未来供给端走势的引导作用。各层次之间形成“周期识别—结构演变—外部驱动”的递进逻辑，为后续供需平衡与价格预测提供基础支撑。|本章节应重点突出供给端周期性与结构性变化的双重逻辑，避免与“行业整体发展概况”章节中对行业周期的宏观描述重复。分析应基于数据趋势的合理推演，而非短期波动。行文需保持高度专业性，以“总—分”结构展开，先总述供给端变化的整体态势，再分述周期、结构、外部因素三方面，

## Deep research
### 搜索

是否训练？是  
训练的数据集是怎么来的？R1+人工+自动方法（对于模型生成的query，我们不知道好坏，因为好坏一个是和输入要相关，一个是适配搜索接口，前者好做；后面通过自动方式做：*让模型改写query，多个改写query进行检索，大模型判断哪个结果更好，作为最终的结果*）  
数据集样本数？500 SFT  
训练是怎么做的？第一阶段SFT  
输入和输出是什么？输入是用户写作要求 + 写作逻辑 + 上一个阶段反馈的缺失信息，输出不超过5个的query

亮点： 1. 有了query之后，反向构建短think，目的是压缩模型生成时间  
**QA**  
问1. 为什么没有做DPO？你已经有了好数据和坏数据  
答1. 好坏数据之间的差距不好，DPO效果不理想；再者通过上述改写行为，已经将往好query走的趋势给了模型潜在学习

### 总结

是否训练？是  
训练的数据集是怎么来的？R1蒸馏+幻觉检查  
数据集样本数？500 SFT  
训练是怎么做的？第一阶段SFT  
输入和输出是什么？输入是当前阶段找到的所有素材，输出总结的多个有用、相关的片段

亮点：加速、先embedding（query 和 素材前150个字）再给模型进行总结。 + 反向构建短think  
**QA**  
问1. 为什么要做总结？  
答1. 提升模型速度，在多轮deep的过程中，基于之前的信息来生成后续的结果，前文信息越短、越简练越好。

### 检查

是否训练？是  
训练的数据集是怎么来的？R1蒸馏  
数据集样本数？500 SFT  
训练是怎么做的？第一阶段SFT  
输入和输出是什么？输入是研究目的 + 当前阶段找到的所有素材，输出总结的判断素材是否满足写作 + 当前还缺失什么素材？

亮点：1. 广度缺失信息 + 深度挖掘信息 两类缺失信息的输出；其中广度缺失是满足 研究目的 的内容要求、深度挖掘信息是 提升写作的深度，让模型知道数据变化的原因，提升写作的写作深度、有更多的思考。

## 正文生成

是否训练？是  
训练的数据集是怎么来的？R1蒸馏+幻觉检查 + 通过真实研报反向构建大量的素材 
数据集样本数？1500 SFT（2：1）  + 100DPO（幻觉DPO）
训练是怎么做的？第一阶段SFT  
输入和输出是什么？输入是写作要求 + 研究目的 + 当前阶段找到的所有素材，输出正文

亮点：1. 如何做幻觉检查？：首先，让模型输出数据、事件、事物的错误和素材不匹配的地方，然后过滤掉。但我们也尝试进行修改，告诉模型你现在出现了xxx问题，再下一次的生成过程中不要出现xxx问题，有70%的修正率。  
2. 通过正文反向构建大量的素材：保留正文中观点等逻辑信息不加入到素材中，素材仅有事件、数据、政策等等，期望让模型能通过学习自发地生成逻辑和观点。
**QA**  
问1. 并行写作的系统，如何保证段间内容不重复  
答1. a 在开发侧，控制检索素材的不重复；
b 在研究目的侧，通过前述前后章节信息的冲突描述，让模型聚焦本章节核心内容，防止在逻辑上和后文产生冲突；
c 正文输入，进行冲突检查，针对有冲突的地方，文章从上到下，依次进行处理（改写【直接改写会导致文本质量下降】、检索再改写），这个方案太慢没有上线；

## 可视化
输入：正文 -> 金融指标搜索query -> 金融指标 -> 聚类（将合适的指标画在一张图中）

step1：query生成
step2：聚类

step1的数据集：和deep research中的query的方案是一样的。
SFT

step2的数据集: 通过正文中的图表反向构建
SFT

亮点：
1. step1尝试使用GRPO强化学习算法  
奖励函数为：  
结果如何：  
中间的一些发现：  

## 评价系统
对比 + 打分，单纯打分和人类的一致性太低

# Agent灵活写作【未来的研报写作方案】
设计短think + tool call的模式，需要设计好的tool + 好的memory管理。  
这个模式下，模型会更多更频繁地进行think和工具调用，这个Agent会更加智能、迭代方便，为长期的强化学习做铺垫。

## TODO List（仿纳米、mutli agent）

## Agent有那些（TODO规划Agent、search Agent、data Agent、write Agent）

## 记忆管理
仿JINA，存储多个节点信息。未来可能会没有memory，直接将path过程中的think obs直接作为上下文以进行最终的正文生成。


# 为什么要找工作（去大的平台，有好的发展），对业务的看法（做机器翻译现在没有了因为大模型，感悟：持续跟踪最前沿的技术，业务上使用好的技术，如果新技术效果好，我们就迭代、保持对新技术的尝试）
