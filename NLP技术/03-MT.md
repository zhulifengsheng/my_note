[TOC]

# Exposure bias

decoder预测单词，在**训练和预测**的时候，是从不同的分布中推断出来的，这种不一致就是exposure bias

Teacher-force技术在训练时受到了ground-truth的约束所以收敛得更快，但是也扼杀了翻译多样性。同时，还会产生 overcorrect（矫枉过正）的现象，如：

> 1. 待生成句的Reference为: "We should comply with the rule."
> 2. 模型在解码阶段中途预测出来："We should abide"
> 3. 然而Teacher-forcing技术把第三个ground-truth "comply" 作为第四步的输入。那么模型根据以往学习的pattern，有可能在第四步预测到的是 "comply with"
> 4. 模型最终的生成变成了 "We should abide with"
> 5. 事实上，"abide with" 用法是不正确的，但是由于ground-truth "comply" 的干扰，模型处于矫枉过正的状态，生成了不通顺的语句。



解决方法：

1. Scheduled Sampling：在训练的时候，每个token以p概率使用 teacher-forcing，1-p概率选择使用Autoregressive。
2. 前期p大，加速收敛；后期p小，让模型在自回归训练中尽可能修复自身产生的错误。



因为这样的方式破坏了Transformer的并行性，所以论文设计了two-pass的解码方案：

据此，论文中设计了一个 two-pass 的解码方案：

1. 在每个training-steps，第一趟先利用teacher-forcing技术，计算出当前句子中每个解码位置所有单词的分数（logits）
2. 根据一定的概率 ![[公式]](https://www.zhihu.com/equation?tex=p) ，选择第二趟解码时，是否用第一趟生成的单词作为decode输入，还是沿用ground-truth作为输入（只有第二趟解码会进行back-propogation）
3. 如果选择第一趟生成的结果，那么每个位置根据预测单词的分数（logits），可以有以下操作：

- 利用 argmax 选择每个位置中分数最大的单词，作为输入。
- 利用分数进行加权平均得到一个embedding向量，作为输入。
- 取topk结果，利用分数进行加权平均得到一个embedding向量，作为输入。
- 根据分数进行多项式采样，作为输入。



**为什么大规模语料训练通用模型的情况下，Exposure-Bias 的影响并不明显（指标优化同步），而在某些特定domain语料finetune的时候凸现出来了呢？**

Domain专业领域的翻译语料中，和通用语料的相比，**通用语料的翻译比较简单，而Domain翻译语料中存在着大量的意译、总结性翻译等问题。** 变数更多，对于机器翻译来说难度更大。正是因为难度的增大，在学霸的关怀下，表面来看还是蒸蒸日上，繁荣发展。但是在脱离学霸后，由于难度增大，Exposure-Bias的问题一下就凸现出来了。



## ACL 2019 best paper

ACL19 best paper 冯洋老师的工作：

Oracle word selection

![image-20220616130415783](C:/Users/zhu/Desktop/Others/office/my_note/神经网络模型/fengyang-work.png)

Oracle sentence selection

选择BLEU最大的译文，作为ground-truth



# 译文长度的控制

BLEU值的评价指标会有一个长度惩罚的选项，如果我们的模型可以生成一个长度合适的译文，往往会得到一个更高的BLEU分数。那么，如果得到良好的译文长度呢？

FAIRseq提供了一个非常简单的方法，调整token <eos>的生成概率

```python
eos_scores /= (step + 1) ** self.len_penalty
```



# NAT



# 对比学习

## 多语言机器翻译

Contrastive Learning for Many-to-many Multilingual Neural Machine Translation

将正确的平行句子（译文）作为正样本；随机句子作为负样本【用encoder输出的表示来做对比学习】

## 解决暴露偏置（ICLR 2021）

Contrastive Learning with Adversarial Perturbations for Conditional Text Generation

在表示空间进行修改得到难度更大的正负样本【token层面的修改太简单了】

正负例构建来源：

1. 表示空间（embedding space）近，语义空间（semantic space）远的负例
2. 表示空间远，语义空间近的正例

负例的构造：

对于decoder输出$H$，加一个扰动得到$\hat H = H + \delta$，扰动来自于对$H$进行求导得到的梯度，沿梯度进行参数更新使得P(y|x)变大【梯度的方向是目标函数上升的方向】；而我们的目标是使语义空间变远，所以往梯度反方向修改。同时，因为这个扰动很小，所以表示空间依然很近。

下图中：$\delta = -\epsilon\frac{g}{||g||_2}$

![](cl2-1.jpg)

正例的构造：

![](cl2-2.jpg)

我们添加一个使对比学习loss上升的扰动【对比学习loss会拉近source的encoder表示和target的decoder表示，这一个扰动会使得表示空间变远】，再添加一个使KL散度最小化的扰动【KL散度越小语义空间越近】

最后，用source encoder的表示做锚，目标语decoder的表示做正负例，进行CL学习。



# 预训练机器翻译

现在的预训练MT，主要做的是多语言MT和无监督MT，纯无监督和低资源MT已经很少见了，都用预训练来做了

1. XLM
2. MASS
2. mBART
2. ALM



# Tricks

## 设计attention mask增强编码能力

<img src="mt-trick1.jpg" style="zoom:67%;" />
