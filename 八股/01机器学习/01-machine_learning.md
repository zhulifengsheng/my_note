[TOC]

# 机器学习的各种任务

## 有监督学习

- 回归任务：给一个样本算出一个值
  - 线性回归：算出一个数
  - 逻辑回归：算出一个0-1之间的数

- 二分类、多分类任务：给一个样本分到一个类别中
- 多标签分类任务：给一个样本打上多个标签

## 无监督学习

- 聚类：将相似的样本聚到一个类别中
- 主成分分析(PCA)：用少量的特征来表示数据集最多的信息，即：删去多余的重复的特征、建立尽可能少的新特征，并使得这些新特征之间两两不相关【最大方差理论】
- 概率图模型：能否发现数据之间的关系
- 生成对抗网络(GAN)：为我们提供一种合成数据的方法



# 概率论与数理统计

## 似然函数

似然函数是关于模型参数的函数，给定输出$y$，关于参数$\theta$的似然函数在数值上等于给定参数$\theta$，得到输出$y$的概率，即$L{ \left(\theta \left|y \right. \right) }=P{ \left(Y=y|\theta \right) }$。考虑下式中的一个情况，
$$
若L(\theta_1|y) = P_{\theta_1}(Y=y) > P_{\theta_2}(Y=y) = L(\theta_2|y)
$$
那么，似然函数就可以反应出这样一个朴素的推测：在参数$\theta_1$下，随机变量$Y$取到值$y$的**可能性**<span style='color:red;'>大于</span>在参数$\theta_2$下，随机变量$Y$取到值$y$的**可能性**。换句话说，我们有理由相信：相对于$\theta_2$，$\theta_1$更真实。

对于一个数据集，似然函数为样本们的联合概率密度，假设数据集中每个样本$x_i$互相独立，则联合概率密度为每个样本的概率密度乘积：
$$
likelihood(\theta) = f(X|\theta) = \prod_{i=1}^Nf(x_i|\theta)
$$
为了简化计算，我们通常取对数转换为加法形式：
$$
\log likelihood(\theta) = \log f(X|\theta) = \sum_{i=1}^N \log f(x_i|\theta)
$$
如果一个损失函数可以使似然函数最大，那么就等同于用极大似然估计的方法来求解模型参数$\theta$



# 信息论

## 熵

熵衡量了一个概率分布的随机性程度，假设随机变量$X$取值为$x$的概率为$p(x)$，而$p(x)$很小但它发生了，则表明它包含的信息量很大。

考虑下面两个随机事件：

1. 明天要下雨
2. 明天世界末日

显然后者的信息量更大，因为一个小概率事件将会发生。

现在，我们定义一个函数$h(x)$来表示随机变量取值为$x$时的信息量大小，则**$h(x)$应为$p(x)$的单调减函数**。同时，考虑一下两个独立的随机变量的情况，$X$和$Y$分别取值为$x$和$y$，它们的联合概率为$p(x, y) = p(x)p(y)$，而它们的信息量函数$h(x,y)$应为$x$信息量和$y$信息量之和，即$h(x,y) = h(x) + h(y)$。于是，我们将熵定义为如下公式：
$$
h(x) = -\ln p(x)
$$
离散随机变量$X$的熵定义为期望:
$$
h(X) = \sum_{i=1}^{n}p(i)h(i) = -\sum_{i=1}^{n}p(i)\ln p(i),
$$

## KL散度（相对熵）

如果一个随机变量$X$有两个概率分布，那么相对熵可以反映这两个概率分布的差异。其值越大，说明这两个概率分布的差异也越大。【注意：KL散度具有非对称性】
$$
\begin{aligned}
KL(p||q) &= \sum_{x} p(x) \ln \frac{p(x)}{q(x)} \\
&= H(p,q) - H(p)
\end{aligned}
$$
如果$p(x)$和$q(x)$完全相同，那么KL散度等于0。在机器学习中，通常以概率分布$p(x)$【真实分布】为目标，拟合一个概率分布$q(x)$【模型预测】来近似它。KL散度反映了我们还需要多少的“信息增量”，才能让$q(x)$达到和$p(x)$一样的效果。【也可以理解为从事件p的角度来看，事件q有多大的不同】

因为，$H(p)$是不变的，所以我们可以直接用交叉熵$H(p,q)$来作为优化目标。

## 交叉熵

和KL散度一样，交叉熵也可以用来衡量真实数据分布$p(x)$与模型预测分布$q(x)$的相似性，设$p(x)$和$q(x)$是两个概率分布的概率密度函数，交叉熵定义为：
$$
H(p, q) = - \sum_{x}p(x)\ln q(x)
$$

## JS散度

JS散度是KL散度的一种变形，解决了KL散度非对称的问题。但JS散度值有一个缺陷：当两个分布完全不重叠时，无论两个分布的中心距离有多近，其JS散度值都是一个常数，这会导致梯度为0。
$$
\begin{aligned}
JS(p||q) &= \frac{1}{2} KL(p||m) + \frac{1}{2} KL(q||m) \\
m(x) &= \frac{1}{2}(p(x) + q(x))
\end{aligned}
$$

## 互信息

互信息定义了两个随机变量的依赖程度【注：这两变量不是独立的】，对于两个离散型随机变量$X$和$Y$，它们之间的互信息定义为：
$$
\begin{aligned}
I(X, Y) &= \sum_{x} \sum_{y} p(x,y) \ln \frac{p(x,y)}{p(x)p(y)} \\
I(X, Y) &= H(X) + H(Y) -- \sum_{x} \sum_{y}p(x,y) \ln p(x,y)【联合熵】
\end{aligned}
$$
如果两个随机变量相互独立，则互信息为0。