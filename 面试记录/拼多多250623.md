# 研报写作的评价方法，评价的公式是什么？

常问：怎么和真实研报进行对比的，差距是多少？  
常问：怎么和竞品【Wind】做对比的，差距是多少？

## 人工评价 + 自动评价结合的方式进行，并在研报写作的多个环节进行评价

## 对于人工评价，我们主要评价大纲和最终写出来的正文  
对于正文评价，定义了多个维度进行打分（紧扣题意、事实正确、逻辑清晰【一个段或一个章节写的内容是一个整体】、分析深度、时效性）  
对于大纲评价，定义了多个维度进行打分（紧扣题意、顺序正确、符合用户写作要求、内容完整）

## 对于自动评价  
### 大纲和正文【ref来自真实研报】  
我们利用R1、GPT-o1共同进行维度打分+比较打分，PROMPT中会给予一个真实研报作为标注答案，让模型生成的结果根据参考答案进行比较
score1 = avg(score_dim)  
score2 = count(result2 > result1) / total

**为什么不单独进行维度打分?因为模型打得不准和人的一致性很差，加上对比之后会好很多**

----

以下都是deep research各个环节的评价，主要是判断deep research是否有金融领域的知识偏移。   
### query生成【ref来自采样人工标注】  
也会提供ref，**因为query是非常重要的，一方面我们使用的是公司自己的搜索接口对query有要求；另一方面要检测生成的几个query是否是重要的、从文本中提取的核心query**

每个query的要求以适配公司接口：
1. 原子性、简单不复合   
2. 主体和搜索词分开（生猪 销量 好于 生猪销量）  
3. 时间信息作为接口参数，而不出现在query中

判断query是否重要为什么一定需要ref？因为人从一段文字中，结合自己的知识，明白应该搜索哪些关键信息；而模型在这方面表现得一般，模型注重全面性，而没有识别、突出重点的能力。  

4. query是否抓住了重点


整体query的要求：  
1. 区分度大   
2. 整体和ref的差距

score = 加权avg(每个query的打分(好、中、差)) + 整体(好、中、差)

### 有用信息总结【ref来自采样人工标注】 
用大模型打分，从总结得是否全面（好、中、差） 和 总结得是否与研究目的相关进行判断（好、中、差）

**提供ref，进行拔高能力的测试**  
模型能否聚合多个相关素材，形成有逻辑、有观点、有数据支撑的有用知识
得到和ref差距的分数（0-5）

### missing information【没有ref】  
用大模型打分，missing是否全面（能写5个但是只写了3个）（好、中、差）、 广度missing是否与研究目的相关（好、中、差）、深度missing是否与当前写作素材相关（好、中、差）

# Transformers进行一层的数学公式是什么？

见03-Transformer

# LeetCode33题 旋转数组找target

```python
def find(nums, target):
    left, right = 0, len(nums)-1
    while left <= right:
        mid = left + (right - left) // 2
        if nums[mid] == target:
            return mid
        
        # 在左侧区间
        elif nums[mid] > nums[-1]:
            if nums[0] <= target < nums[mid]:
                right = mid - 1
            else:
                left = mid + 1
        # 在右侧区间
        else:
            if nums[mid] < target <= nums[-1]:
                left = mid + 1
            else:
                right = mid - 1

    return -1
```