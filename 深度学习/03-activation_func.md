[TOC]

# 激活函数

激活函数提供网络的非线性建模能力，如果没有非线性变化，那么再多的线性变换也是单层线性变换无疑。

## sigmoid

缺点：

1. 饱和性，输入的绝对值过大时，导数趋近于0，就是导致梯度消失

2. sigmoid函数的输出不是0均值（0-1），导致上层神经元得到了非0均值的输入信号。【通过normalization可以进行改进】 

   例如$x>0, f=wx+b$【$x为sigmoid函数输出$】那么对$w$求局部梯度则都为正，这样在反向传播的过程中$w$会一直往一个方向更新，导致一种偏移的效果



## tanh

$$
y = \frac{e^x-e^{-x}}{e^x+e^{-x}}
$$

优点：输出在-1-1之间，解决了输出0均值问题

缺点：饱和性



## relu

优点：计算速度很快，计算了梯度消失的问题（没有饱和区）

缺点：1. 输出不以0为中心 2. 有一些神经元可能永远都不会被激活



## gelu

GELU希望在激活中加入正则化dropout的思想。
$$
gelu(x) = \Phi(x)\times x + (1-\Phi(x)) \times 0x = x\Phi(x) 
$$
$\Phi(x)$是标准正态分布的累积分布函数，这说明x越小，它被丢弃的可能性越大。