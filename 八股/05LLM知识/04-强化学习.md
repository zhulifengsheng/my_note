先明确一个概念——策略（policy），它就是RLHF中的“学生”，policy由两个模型组成，一个叫做演员模型（Actor）【演员就是我们想要训练出来的大模型。】，另一个叫做评论家模型（Critic）【评论家是强化学习的辅助模型】，一个负责决策，一个负责总结得失。Reward model则是RLHF中的“老师”，指导policy（“学生”）的更新。

强化学习的三个过程：采样 -> 反馈 -> 学习

# PP0
> https://zhuanlan.zhihu.com/p/645225982

```python
'''
ref_policy_model —— 基准模型（原模型，不更新参数）
policy_model —— Actor（输入一段上下文，它将输出下一个token的概率分布） + Critic（评论家是强化学习的辅助模型，输入一段上下文，它将输出下一个token的“收益”。）
reward_model —— reward模型
'''

policy_model = load_model()
ref_policy_model = policy_model.copy() # 基准模型

for k in range(20000):
    # 采样m个prompt
    prompts = sample_prompt()
    ## response：m个字符串，每个字符串包含n个token
    ## old_log_probs：演员输出的 m x n 的张量，包含了response中每次生成token的概率的对数 
    ## old_values：评论家输出的 m x n 的张量，包含了response中每次生成token时评论家Critic预估的收益
    responses, old_log_probs, old_values = respond(policy_model, prompts)
    
    # 反馈（计算奖励）
    ## 奖励大模型【老师】（reward_model） 对 问答序列（data） 进行打分，指导模型的改进方向
    ## scores：奖励模型输出的 m x 1 的张量，仅在最后一个位置上生成整个prompt+response的奖励
    scores = reward_model(prompts, responses)
    ref_log_probs, _ = ref_policy_model.forward_pass(prompts, responses)
    ## ref_log_probs 被用来和old_log_probs，计算KL散度然后增加到reward中，防止模型更新偏离原始模型太多
    rewards = reward_func(scores, old_log_probs, ref_log_probs)
    
    # 学习（更新参数）
    ## advantages（优势） = return（实际收益）【从生成第j个token开始到生成第N个token为止，所能获得的所有奖励的总和。】 - old_values（预期收益）
    advantages = advantage_func(rewards, old_values)
    returns = advantages + old_values

    for epoch in range(epoch):
        log_probs, values = policy_model.forward_pass(prompts, responses)
        ## actor_loss用来训练Actor，在advantages为正的地方，让模型学习增大输出该token的概率
        actor_loss = actor_loss_func(advantages, old_log_probs, log_probs)
        ## critic_loss用来训练Critic，让它输出的预期收益(values)逼近实际收益(returns)
        critic_loss = critic_loss_func(returns, values)

        ## 合并loss
        loss = actor_loss + 0.1 * critic_loss
        ## 在实际过程中，actor和critic来自同一个LLM，评论家就是将演员模型的倒数第二层连接到一个新的全连接层上。除了这个全连接层之外，演员和评论家的参数都是共享的。因此更新policy_model即可
        train(loss, policy_model.parameters())
```

## Loss
Actor Loss:  
$a = return(来自奖励模型) - old\_value(来自评论家模型)$

$\text { actor\_loss }=- \frac{1}{N} \sum_{j=1}^{N} a[j] \times p(\text { token }[\mathrm{j}] \mid \text { prompt })$  

当优势大于0时，概率越大，loss越小；因此优化器会通过增大概率（即强化优势动作）来减小loss  
当优势小于0时，概率越小，loss越小；因此优化器会通过减小概率（即弱化劣势动作）来减小loss

Critic Loss:  
$\text { critic\_loss }= \frac{1}{2N} \sum_{j=1}^{N} (return[j] - value[j])^2 $ 


## 采样
模型接受prompt，生成response。

## 反馈
prompt + response送入奖励模型得到奖励分数。其中还会利用训练模型（Actor）和基准模型计算KL散度，防止模型更新太大。

## 学习
**强化优势动作**是PPO学习阶段的焦点。在深入探讨之前，我们首先要明确一个关键概念——优势。此处，我们将优势定义为“实际获得的收益超出预期的程度”。【优势 = 实际收益 - 预期收益。】

对于LLM而言，为第i个response生成第j个token的实际收益就是：从生成第j个token开始到生成第N个token为止，所能获得的所有奖励的总和。我们用return来表示实际收益。预期收益又该如何计算呢？记得我们在“采样”阶段提到过，policy包含演员模型和评论家模型，其中后者是用来预估收益的。其实，当时说的收益old_values就是现在我们想要计算的预期收益。评论家会为response中的每个token计算一个预期收益，第i个response的第j个token的预期收益记为values[i, j]

**强化优势动作**表示：如果在上下文（context）中生成了某个token，并且这个动作的优势很高，那么我们应该增加生成该token的概率。

# DPO
DPO相比于PPO是通过引入人类偏好数据，将在线策略优化，修改为通过二元交叉熵直接拟合人类偏好数据的**离线策略**，不再属于强化学习范畴，**属于有监督学习**。

## 与PPO相比优缺点
优点：
1. 不需要奖励模型
2. 稳定性高
3. Loss简单，训练难度低，Loss如下：
$\mathcal{L}_{\mathrm{DPO}}\left(\pi_{\theta} ; \pi_{\mathrm{ref}}\right)=-\mathbb{E}_{\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\mathrm{ref}}\left(y_{w} \mid x\right)}-\beta \log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\pi_{\mathrm{ref}}\left(y_{l} \mid x\right)}\right)\right]$

缺点：
1. 容易过拟合：DPO由于缺少reward model的泛化，因此容易直接拟合人类偏好数据，造成过拟合。
2. 需求更大标注数据量：相比PPO等，DPO的效果表现更依赖标注数据量。
3. 多任务适配较难：由于DPO仅依赖数据，所以如果需要进行多任务的对比，则需要从头标注涉及到多个维度的数据，但是在线策略的方法可以通过单个维度的数据，训练不同的多个reward model，引入多维度的奖励。

## 流程
1. 对每个prompt，采样两个结果并进行人工标记偏好顺序，构建离线偏好数据集
2. 通过最小化Loss，优化LM

# KTO


# GRPO


