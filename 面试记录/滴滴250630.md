
# GRPO

# PPO

# KTO

# DPO

# KIMI-RESEARCHER是怎么做的

> https://moonshotai.github.io/Kimi-Researcher/

利用RL方法，训练了一个端到端的模型，完成自主search和reasoning，而不再是工作流workflow的模式。**端到端代理强化学习训练单一模型来全面解决问题**：给定一个查询，agent会探索大量可能的策略，获得正确解决方案的奖励，并从完整的轨迹中进行学习。与 SFT 不同，它能够自然地处理长时间的、基于策略的推理，并适应不断变化的工具和环境；与模块化方法不同，所有技能（规划、感知和工具使用）都是一起学习的，无需手工制定规则或工作流模板。

![alt text](Kimi-Researcher.png)

#### 三大工具
1. 搜索工具

2. 浏览器工具

3. 代码编写工具

#### 数据集
1. 数学和代码推理：Kimi-Researcher 学习使用工具集来解决此类问题，而不仅仅是单纯地运用思路链。

2. 搜索：Agent必须在上下文约束内迭代搜索、综合和推理才能得出有效答案的场景。通过Case案例，研究发现了这些硬搜索任务如何推动更深层次的规划和强大的工具增强推理策略的出现。

#### RL的结果奖励
1. 格式奖励：如果在推理的路径中包含无效工具调用或上下文/迭代超过最大限制，模型将受到惩罚。

2. 结果正确性奖励：对于没有格式错误的推理路径，奖励基于模型的答案与基本事实之间的比较。

为了鼓励模型发现更短、更有效的探索路径，第i步的reward = r * t^(T-i)，其中 0 < t < 1，T是步骤的数量

#### 上下文管理

长step的推理路径，可能涉及大量的观察上下文（来自于search），而缺乏内存管理的简单智能体很容易在 10 次迭代内就超出限制。为了解决这个问题，我们设计了一种上下文管理机制，允许模型保留重要信息并丢弃不必要的文档，从而将单次推出轨迹的迭代次数扩展到 50 次以上。

#### 涌现的新能力

1. 当出现来自多个来源的相互矛盾的信息时，Kimi-Researcher 会通过迭代假设改进和自我修正来解决不一致问题。

2. Kimi-Researcher 表现出谨慎和严谨：即使对于看似简单的问题，它也会在回答之前刻意进行额外的搜索并交叉验证信息。

