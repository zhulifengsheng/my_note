# 前身COT

COT（chain-of-thought）本身属于prompt工程，通过写出更好的prompt来激发模型回答出正确答案的能力

## COT
> https://arxiv.org/pdf/2201.11903
![COT_example](cot_example.png)
把思考的中间过程详细地写在prompt中，引导模型也进行同样的思考输出，提升推理能力

## COT self-consistency
> https://arxiv.org/pdf/2203.11171
![cot_self-consistency](cot_self-consistency.png)
多模型的结果进行多次采样，采用策略（如投票法），选择最好的结果

## zero-shot COT（Let's think step by step）
> https://arxiv.org/pdf/2205.11916
![Let's think step by step](zero-shot_cot.png)
通过一个简单的命令 let's think step by step，不需要shot就可以让模型进行思考推导，并得到正确答案，减少手工cot的成本

## Auto COT
> https://zhuanlan.zhihu.com/p/17878862043
![Auto COT](auto_cot.png)
从一堆问题集中抽取适合的问题（论文中选择的标准是：简单、问题短、推理步骤少，防止模型推理出错给用户提问一个错误的shot）用let's think step by step引导模型进行解答得到qa对（zero-shot cot），然后将qa对作为shot来完成用户提问的解答

## TOT 
> https://zhuanlan.zhihu.com/p/634180290
> https://arxiv.org/pdf/2305.10601
![tot](tot.png)
1. 将一个问题拆解为多个阶段来进行回答
2. 模型每个阶段生成多个样本
3. 评价模型上一步生成的哪个样本最好

## 引申：如何在写作中引入COT，提升正文质量
idea：用r1造数据集对<think>部分进行人工校对，然后训练自己的模型，使其拥有高质量的think（推理）能力

## “Wait”注入
> https://github.com/simplescaling/s1?tab=readme-ov-file#vllm-with-budget-forcing
> https://zhuanlan.zhihu.com/p/21602993558  

通过注入wait，延长模型的思考，变相增加推理时间
```python
o = model.generate(prompt)

ignore_str = "Wait" # 注入词wait
max_tokens_thinking_tmp = MAX_TOKENS_THINKING
if max_tokens_thinking_tmp > 0:
    for i in range(NUM_IGNORE): # 注入几次
        max_tokens_thinking_tmp -= len(o[0].outputs[0].token_ids)
        prompt += o[0].outputs[0].text + ignore_str # prompt拼接模型返回+wait
        o = model.generate(prompt)  # 模型看见wait会进行反思，生成更好的结果
```


# o1最初的推理模型

> https://zhuanlan.zhihu.com/p/720106482

引入 推理时计算 *test-time compute* 概念

推理模型的主要创新点：模型可以自己提出假设、验证假设、进行反思（如果假设结果错误）【模型具有质疑自己前文推断错误的能力】

openai声明使用了RL技术来实现o1

## RL技术如何赋予模型推理能力
### SFT走到了尽头，训练数据几乎用光
> scaling law C（计算量flops）=6N（模型参数量）*D（token）
> 当模型参数量不变时，训练的数据越多，模型能力越强
> https://zhuanlan.zhihu.com/p/20966132534

SFT的问题：
1. 需要大量的高质量数据，难以应用到垂类领域
2. 无法使用负样本

### RLHF
| 技术 | 优点|缺点|模型|
| --- | ---|---|---|
| RLHF	| 1. 可以对齐人类偏好及价值观<br />2. 能利用错误数据<br />3. 数据利用效率高 |1. 偏好建模困难，容易hacking<br/>2. 训练成本高 |ChatGPT|
| Self-play	| 1. 绝对强度更高，甚至超越最强人类、专家<br/>2. 可以实现双人零和博弈的最优 |1. 有时候无法理解人类，行为不像人<br/>2. 训练及推理成本极高 |o1|

#### PPO
xxxx


> RLHF解读

### Self-paly（o1所使用的技术）
generator 和 verifier（验证器）
强大的verifier是self-paly的基础
generator 和 verifier交互对抗，互相增强

# r1的解法

## 训练过程
