[TOC]

# 激活函数

激活函数提供网络的非线性建模能力，如果没有非线性变化，那么再多的线性变换也是单层线性变换无疑。

## sigmoid

缺点：

1. 饱和性，输入的绝对值过大时，导数趋近于0，就是导致梯度消失
2. sigmoid函数的输出不是0均值（0-1），例如$x>0, f=wx+b$那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种偏移的效果



## tanh

$$
y = \frac{e^x-e^{-x}}{e^x+e^{-x}}
$$

优点：输出在-1-1之间，解决了输出0均值问题

缺点：饱和性



## relu

优点：计算速度很快，计算了梯度消失的问题（没有饱和区）

缺点：有一些神经元可能永远都不会被激活