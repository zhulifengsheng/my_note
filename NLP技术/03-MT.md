[TOC]

# exposure bias

decoder预测单词，在**训练和预测**的时候，是从不同的分布中推断出来的，这种不一致就是exposure bias

Teacher-force技术在训练时受到了ground-truth的约束所以收敛得更快，但是也扼杀了翻译多样性。同时，还会产生 overcorrect（矫枉过正）的现象，如：

> 1. 待生成句的Reference为: "We should comply with the rule."
> 2. 模型在解码阶段中途预测出来："We should abide"
> 3. 然而Teacher-forcing技术把第三个ground-truth "comply" 作为第四步的输入。那么模型根据以往学习的pattern，有可能在第四步预测到的是 "comply with"
> 4. 模型最终的生成变成了 "We should abide with"
> 5. 事实上，"abide with" 用法是不正确的，但是由于ground-truth "comply" 的干扰，模型处于矫枉过正的状态，生成了不通顺的语句。



解决方法：

1. Scheduled Sampling：在训练的时候，每个token以p概率使用 teacher-forcing，1-p概率选择使用Autoregressive。
2. 前期p大，加速收敛；后期p小，让模型在自回归训练中尽可能修复自身产生的错误。



因为这样的方式破坏了Transformer的并行性，所以论文设计了two-pass的解码方案：

据此，论文中设计了一个 two-pass 的解码方案：

1. 在每个training-steps，第一趟先利用teacher-forcing技术，计算出当前句子中每个解码位置所有单词的分数（logits）
2. 根据一定的概率 ![[公式]](https://www.zhihu.com/equation?tex=p) ，选择第二趟解码时，是否用第一趟生成的单词作为decode输入，还是沿用ground-truth作为输入（只有第二趟解码会进行back-propogation）
3. 如果选择第一趟生成的结果，那么每个位置根据预测单词的分数（logits），可以有以下操作：

- 利用 argmax 选择每个位置中分数最大的单词，作为输入。
- 利用分数进行加权平均得到一个embedding向量，作为输入。
- 取topk结果，利用分数进行加权平均得到一个embedding向量，作为输入。
- 根据分数进行多项式采样，作为输入。



**为什么大规模语料训练通用模型的情况下，Exposure-Bias 的影响并不明显（指标优化同步），而在某些特定domain语料finetune的时候凸现出来了呢？**

Domain专业领域的翻译语料中，和通用语料的相比，**通用语料的翻译比较简单，而Domain翻译语料中存在着大量的意译、总结性翻译等问题。** 变数更多，对于机器翻译来说难度更大。正是因为难度的增大，在学霸的关怀下，表面来看还是蒸蒸日上，繁荣发展。但是在脱离学霸后，由于难度增大，Exposure-Bias的问题一下就凸现出来了。



ACL19 best paper 冯洋老师的工作：

Oracle word selection

![image-20220616130415783](C:/Users/zhu/Desktop/Others/office/my_note/神经网络模型/fengyang-work.png)

Oracle sentence selection

选择BLEU最大的译文，作为ground-truth



# 译文长度的控制

BLEU值的评价指标会有一个长度惩罚的选项，如果我们的模型可以生成一个长度合适的译文，往往会得到一个更高的BLEU分数。那么，如果得到良好的译文长度呢？

FAIRseq提供了一个非常简单的方法，调整token <eos>的生成概率

```python
eos_scores /= (step + 1) ** self.len_penalty
```



# NAT



# 对比学习

## 多语言机器翻译

## 解决暴露偏置



# 无监督机器翻译

https://zhuanlan.zhihu.com/p/66957485

XLM跨语言预训练模型，得到一个可以编码任意语种句子的Transformer模型，这和无监督机器翻译中“表示空间对齐”的概念不谋而合。

XLM丢去了NSP，将segment embedding换成了language embedding。

## 全语种混合BPE

为了防止，小语种数据过少，进行加温度的多项式分布采样。$q_i$为这个i语种的句子被选择的概率；$p_i$为这个i语种语料占全部语料的比例。
$$
q_i = \frac{p_i^\alpha}{\sum_{j=1}^Np_j^\alpha}\space with\space p_i=\frac{n_i}{\sum_{k=1}^Nn_k}
$$

## 预训练任务

1. CLM 基本的语言模型任务，计算PPL
2. MLM
3. TLM 利用双语平行句子，借助译文，预测源文中被MASK的词；反之亦然

## finetune

用预训练好的参数初始化encoder和decoder，进行有监督MT、无监督MT等任务
